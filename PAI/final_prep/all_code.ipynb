{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d0747f2",
   "metadata": {},
   "source": [
    "# linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e283607",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "df = {\n",
    "    \"weight\": [50,55,60,63,68,72,75],\n",
    "    \"height\": [150,155,160,165,170,175,180]\n",
    "}\n",
    "df = pd.DataFrame(df)\n",
    "\n",
    "\n",
    "\n",
    "X = np.array(df['height']).reshape(-1,1)\n",
    "\n",
    "y = np.array(df['weight'])\n",
    "# ------------------------\n",
    "\n",
    "# plot data\n",
    "# ------------------------\n",
    "plt.scatter(X,y)\n",
    "plt.xlabel(\"weight\")\n",
    "plt.ylabel(\"height\")\n",
    "# ------------------------\n",
    "# convert X mxn to X mxn+1\n",
    "X = np.c_[np.ones((len(X),1)),X]\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# perform OLS\n",
    "w = np.linalg.inv(X.T@X)@X.T@y\n",
    "y_p = X@w\n",
    "# ------------------------\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(df.iloc[:,1],y_p)\n",
    "\n",
    "## height: 172, weight =?\n",
    "\n",
    "print(\"weight:\")\n",
    "print(np.array([1,172])@w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb2294a",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b644ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "\n",
    "df['Sex'] = df['Sex'].map({'male':0,'female':1})\n",
    "df['Age'].fillna(df['Age'].median(), inplace=True)\n",
    "df['Fare'].fillna(df['Fare'].median(), inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "X = df[['Pclass','Sex','Age','Fare']] # features\n",
    "y = df['Survived'] # output\n",
    "\n",
    "\n",
    "# ---\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "# ---\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
    "tree.fit(X_train, y_train)\n",
    "# ---\n",
    "from sklearn.tree import plot_tree\n",
    "plot_tree(tree,feature_names=['Pclass','Sex','Age','Fare'], class_names=['Died', 'Survived'],) # Optional: for better class labels)\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score,precision_score\n",
    "# now train a model to predict the data, to test accuracy\n",
    "y_p = tree.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test,y_p)\n",
    "\n",
    "precision = precision_score(y_test, y_p, pos_label=1, zero_division=0)\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a6c9eb",
   "metadata": {},
   "source": [
    "# kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc437f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df =pd.read_csv(\"https://raw.githubusercontent.com/SteffiPeTaffy/machineLearningAZ/refs/heads/master/Machine%20Learning%20A-Z%20Template%20Folder/Part%204%20-%20Clustering/Section%2025%20-%20Hierarchical%20Clustering/Mall_Customers.csv\")\n",
    "\n",
    "print(df)\n",
    "X = df.iloc[:,[3,4]].values\n",
    "print(X)\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# --- 1. Simulate the Mall Customer Data ---\n",
    "# Simulating data that is known to form 5 distinct clusters (Mall Customer dataset structure)\n",
    "np.random.seed(42)\n",
    "n_samples = 200\n",
    "\n",
    "# Cluster 1: High Income, Low Spending (Low Value)\n",
    "inc_1 = np.random.uniform(70, 140, 40)\n",
    "spn_1 = np.random.uniform(10, 40, 40)\n",
    "\n",
    "# Cluster 2: Medium Income, Medium Spending (Average)\n",
    "inc_2 = np.random.uniform(40, 80, 80)\n",
    "spn_2 = np.random.uniform(40, 60, 80)\n",
    "\n",
    "# Cluster 3: Low Income, High Spending (Careful Spenders)\n",
    "inc_3 = np.random.uniform(10, 40, 25)\n",
    "spn_3 = np.random.uniform(70, 99, 25)\n",
    "\n",
    "# Cluster 4: Low Income, Low Spending (Targetable)\n",
    "inc_4 = np.random.uniform(10, 40, 25)\n",
    "spn_4 = np.random.uniform(10, 40, 25)\n",
    "\n",
    "# Cluster 5: High Income, High Spending (Premium)\n",
    "inc_5 = np.random.uniform(70, 140, 30)\n",
    "spn_5 = np.random.uniform(70, 99, 30)\n",
    "\n",
    "# Combine all data\n",
    "income = np.concatenate([inc_1, inc_2, inc_3, inc_4, inc_5])\n",
    "spending = np.concatenate([spn_1, spn_2, spn_3, spn_4, spn_5])\n",
    "\n",
    "# Create the Feature Matrix X\n",
    "X = np.column_stack((income, spending))\n",
    "\n",
    "\n",
    "# 2. Train the K-Means model with K=5\n",
    "kmeans = KMeans(n_clusters=5, init='k-means++', random_state=42, n_init=10)\n",
    "y_kmeans = kmeans.fit_predict(X)\n",
    "\n",
    "# 3. Visualize the clusters and centers\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Scatter plot for each cluster\n",
    "for i in range(5):\n",
    "    plt.scatter(X[y_kmeans == i, 0], X[y_kmeans == i, 1], \n",
    "                s=50, label=f'Cluster {i+1}')\n",
    "\n",
    "# Plot the cluster centers (Centroids)\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], \n",
    "            s=300, label='Centroids', marker='*')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e2053a",
   "metadata": {},
   "source": [
    "# multi-LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f39cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# --- 1. Data Simulation (Mimicking House Prices Dataset) ---\n",
    "np.random.seed(42)\n",
    "n_samples = 1460 \n",
    "\n",
    "# Features:\n",
    "# OverallQual (Rating: 1-10)\n",
    "OverallQual = np.random.randint(4, 10, n_samples) \n",
    "# GrLivArea (Sq. Ft: typical range 1000-3000)\n",
    "GrLivArea = np.random.normal(1500, 400, n_samples).astype(int)\n",
    "# GarageCars (Number of cars: 1-4)\n",
    "GarageCars = np.random.randint(1, 5, n_samples) \n",
    "# YearBuilt (Year: typical range 1900-2010)\n",
    "YearBuilt = np.random.randint(1950, 2010, n_samples) \n",
    "\n",
    "# Target: SalePrice (Simulated with inherent dependencies + noise)\n",
    "# Model: Price = 10000 + 15000*Qual + 60*Area + 20000*Cars + 500*Year + noise\n",
    "SalePrice = (\n",
    "    10000 + 15000 * OverallQual + 60 * GrLivArea + \n",
    "    20000 * GarageCars + 500 * (YearBuilt - 1950) + \n",
    "    np.random.normal(0, 30000, n_samples)\n",
    ").astype(int)\n",
    "\n",
    "# Combine into a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'OverallQual': OverallQual,\n",
    "    'GrLivArea': GrLivArea,\n",
    "    'GarageCars': GarageCars,\n",
    "    'YearBuilt': YearBuilt,\n",
    "    'SalePrice': SalePrice\n",
    "})\n",
    "\n",
    "# --- 2. Define Features (X) and Target (y) ---\n",
    "X = df[['OverallQual', 'GrLivArea', 'GarageCars', 'YearBuilt']]\n",
    "y = df['SalePrice']\n",
    "feature_names = X.columns\n",
    "\n",
    "# --- 3. Split Data ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# --- 4. Train Multiple Linear Regression Model ---\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# --- 5. Predict and Evaluate ---\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate Metrics\n",
    "r_squared = r2_score(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "# Extract Coefficients\n",
    "coefficients = pd.Series(model.coef_, index=feature_names)\n",
    "intercept = model.intercept_\n",
    "\n",
    "print(\"## üè° Multiple Linear Regression Results\\n\" + \"=\"*45)\n",
    "\n",
    "print(\"\\n### üí∞ Coefficients and Interpretation\")\n",
    "print(f\"Intercept (Base Price): ${intercept:,.2f}\")\n",
    "print(\"\\n--- Coefficients ---\")\n",
    "print(coefficients.map('${:,.2f}'.format))\n",
    "\n",
    "print(\"\\n### üìà Model Evaluation Metrics\")\n",
    "print(f\"R-squared ($\\\\mathbb{{R}}^2$): {r_squared:.4f}\")\n",
    "print(f\"RMSE: ${rmse:,.2f}\")\n",
    "\n",
    "print(\"\\n### üëë Feature Impact Analysis\")\n",
    "# Use the absolute value of the standardized coefficients (or scaled data) \n",
    "# for true impact, but for general intuition, we'll use the relative magnitude \n",
    "# of the unstandardized coefficients as a proxy.\n",
    "# We will identify the largest coefficient (excluding the intercept)\n",
    "max_impact_feature = coefficients.abs().idxmax()\n",
    "print(f\"The feature with the largest coefficient magnitude is: **{max_impact_feature}**.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb373ebf",
   "metadata": {},
   "source": [
    "# feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0063a13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# --- 1. Data Simulation ---\n",
    "np.random.seed(42)\n",
    "n_samples = 500\n",
    "\n",
    "# Feature Simulation: Ensure they are factors of price\n",
    "RAM = np.random.choice([8, 16, 32], n_samples, p=[0.5, 0.35, 0.15]) # 8, 16, 32 GB\n",
    "Weight = np.random.normal(1.9, 0.4, n_samples) # 1.5 to 3.0 kg\n",
    "CPU_freq = np.random.normal(2.8, 0.5, n_samples) # 2.0 to 4.0 GHz\n",
    "Storage = np.random.choice([256, 512, 1024], n_samples, p=[0.4, 0.4, 0.2]) # 256, 512, 1024 GB\n",
    "\n",
    "# Price Simulation: Price = Intercept + Weights * Features + Noise\n",
    "# RAM has high weight, Weight has negative weight, CPU and Storage have moderate weights.\n",
    "Price = (\n",
    "    100 + 70 * RAM + \n",
    "    (-200) * Weight + \n",
    "    150 * CPU_freq + \n",
    "    0.5 * Storage + \n",
    "    np.random.normal(0, 100, n_samples)\n",
    ")\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'RAM': RAM,\n",
    "    'Weight': Weight,\n",
    "    'CPU_frequency': CPU_freq,\n",
    "    'Storage_size': Storage,\n",
    "    'Price': Price\n",
    "})\n",
    "\n",
    "# --- 2. Define X and y ---\n",
    "features = ['RAM', 'Weight', 'CPU_frequency', 'Storage_size']\n",
    "X = df[features]\n",
    "y = df['Price']\n",
    "\n",
    "# --- 3. Split Data (Unscaled) ---\n",
    "X_train_unscaled, X_test_unscaled, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# --- 4. Feature Scaling (StandardScaler) ---\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_unscaled)\n",
    "X_test_scaled = scaler.transform(X_test_unscaled)\n",
    "\n",
    "# Convert back to DataFrame for easy viewing (not strictly necessary for training)\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=features)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=features)\n",
    "\n",
    "# --- 5. Train and Evaluate Unscaled Model ---\n",
    "model_unscaled = LinearRegression()\n",
    "model_unscaled.fit(X_train_unscaled, y_train)\n",
    "y_pred_unscaled = model_unscaled.predict(X_test_unscaled)\n",
    "\n",
    "r2_unscaled = r2_score(y_test, y_pred_unscaled)\n",
    "rmse_unscaled = np.sqrt(mean_squared_error(y_test, y_pred_unscaled))\n",
    "\n",
    "# --- 6. Train and Evaluate Scaled Model ---\n",
    "model_scaled = LinearRegression()\n",
    "model_scaled.fit(X_train_scaled, y_train)\n",
    "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
    "\n",
    "r2_scaled = r2_score(y_test, y_pred_scaled)\n",
    "rmse_scaled = np.sqrt(mean_squared_error(y_test, y_pred_scaled))\n",
    "\n",
    "# Extract scaled coefficients for importance analysis\n",
    "scaled_coefficients = pd.Series(model_scaled.coef_, index=features)\n",
    "\n",
    "# --- 7. Reporting ---\n",
    "print(\"## üíª Laptop Price Prediction Results\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\n### üìà Performance Comparison (Scaled vs Unscaled)\")\n",
    "print(\"Note: For Linear Regression, scaling affects coefficient magnitude, not R¬≤ or RMSE.\")\n",
    "print(\"{:<20} {:<10} {:<10}\".format(\"Model\", \"R-squared\", \"RMSE\"))\n",
    "print(\"{:<20} {:<10.4f} {:<10.2f}\".format(\"Unscaled Features\", r2_unscaled, rmse_unscaled))\n",
    "print(\"{:<20} {:<10.4f} {:<10.2f}\".format(\"Scaled Features\", r2_scaled, rmse_scaled))\n",
    "\n",
    "print(\"\\n### ‚≠ê Most Significant Predictor\")\n",
    "print(\"Coefficients are extracted from the **Scaled Model** as they represent true relative importance.\")\n",
    "print(\"\\n--- Scaled Coefficients (Relative Importance) ---\")\n",
    "print(scaled_coefficients.map('{:.4f}'.format))\n",
    "\n",
    "most_significant_predictor = scaled_coefficients.abs().idxmax()\n",
    "print(f\"\\nMost Significant Predictor (based on absolute scaled coefficient): **{most_significant_predictor}**\")\n",
    "\n",
    "print(\"\\n### üî¨ Unscaled Coefficients (Units of Price per Unit of Feature)\")\n",
    "print(pd.Series(model_unscaled.coef_, index=features).map('{:,.2f}'.format))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
